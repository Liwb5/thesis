task_name: debug.rl.1.0111  # this is the name of the task
device: null    # if you do not want to use GPU, set null
log_level: debug 
use_summaryWriter: False 
seed: 1667
loss: nll_loss
metrics: rouge_metric

optimizer: 
    type: Adam
    args:
        lr: 0.001
    
trainer:
    type: Trainer
    args:
        epochs: 1
        save_period: 10000000000000
        print_loss_every: 20                   # print train loss every [num] batches, related to the batch size
        print_token_every: 1000000000             # print training tokens to see the result. print every [num] batches 
        print_val_token_every: 500000000         # print val tokens to see the result. print every [num] batches 
        do_validation: False 
        max_norm: 1.0
        epsilon: 1.0
        save_dir: checkpoints/
        log_dir: logs/
        teacher_forcing_ratio: 0.9
        output_dir: outputs/

data_loader:
    train_data: ./data/dm_data_from_summaRuNNer/origin_data/fake_data.json
    val_data: ./data/dm_data_from_summaRuNNer/origin_data/val.json
    test_data: ./data/dm_data_from_summaRuNNer/origin_data/test.json
    batch_size: 2
    shuffle: False
    val_data_quota: 100
    data_quota: -1


vocabulary: 
    vocab_file: ./data/dm_data_from_summaRuNNer/origin_data/w2i.json
    sent_trunc: 120
    doc_trunc: 50
    # split_token: \n


lr_scheduler: 
    type: StepLR
    args: 
        step_size: 50
        gamma: 0.1
    
model: 
    type: RL_AE
    args: 
        embed_dim: 100
        vocab_size: 153826
        hidden_size: 128
        max_len: 120
        sos_id: 2
        eos_id: 3
        max_selected: 3
        rnn_cell: gru
        input_dropout_p: 0.0 
        dropout_p: 0.0 
        bidirectional: True
        use_attention: False
        select_mode: distribute 
        sample_num: 3
        pretrained_embed: ./data/cnn_dailymail_data/finished_dm_data/embed 

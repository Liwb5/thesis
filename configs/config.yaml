# multi processing with calculating rewards
# multiply reward with 1000 to make the loss more sensitive
# using rouge_f to calculate reward
# using tanh and explorate_rate==5 to contrain the result of attention alpha, so that sample can explorate
# loss function can avoid nan value 
# using multi sample instead of exponential moving avg reward
# learning rate 0.00001
# 0226: add inferSent to calculate reward instead of rouge metric 
# using new embed file: embed_glove.84B.300d.153826w.npz
# using avg_pool1d instead of max_pool1d in stack encoder
# hidden_size 200
task_name: train.hs200_weight_decay_maxSample_max_pool1d_embed_300d_file_rl_advantage_R_1000Rewards_explorate_rate5_lr0.00001.v2_2.big.0306
device: 0    # if you do not want to use GPU, set null
log_level: info 
use_summaryWriter: True
seed: 1667
loss: nll_loss
metrics: rouge_metric

optimizer: 
    type: Adam
    args:
        lr: 0.00001
        weight_decay: 0.000001
    
trainer:
    type: Trainer
    reward_type: rouge    # inferSent, rouge 
    args:
        epochs: 30
        save_period: 1
        print_loss_every: 20                   # print train loss every [num] batches, related to the batch size
        print_token_every: 1000000000             # print training tokens to see the result. print every [num] batches 
        print_val_token_every: 500000000         # print val tokens to see the result. print every [num] batches 
        do_validation: True 
        max_norm: 1.0
        epsilon: 0.9    # the initial probability of exploration
        save_dir: checkpoints/
        log_dir: logs/
        teacher_forcing_ratio: 0.9
        output_dir: outputs/

data_loader:
    train_data: ./data/dm_data_from_summaRuNNer/origin_data/train.json
    val_data: ./data/dm_data_from_summaRuNNer/origin_data/val.json
    test_data: ./data/dm_data_from_summaRuNNer/origin_data/test.json
    batch_size: 20
    shuffle: True
    val_data_quota: 10000
    data_quota: -1


vocabulary: 
    vocab_file: ./data/dm_data_from_summaRuNNer/origin_data/w2i.json
    embed_file: ./data/dm_data_from_summaRuNNer/origin_data/embed_glove.840B.300d.153826w.npz
    sent_trunc: 120
    doc_trunc: 50


lr_scheduler: 
    type: StepLR
    args: 
        step_size: 50
        gamma: 0.1
    
model: 
    type: RL_AE
    args: 
        embed_dim: 300
        vocab_size: 153826
        hidden_size: 200
        max_len: 120
        sos_id: 2
        eos_id: 3
        max_selected: 4
        rnn_cell: gru
        input_dropout_p: 0.0 
        dropout_p: 0.0 
        bidirectional: True
        use_attention: False
        select_mode: distribute 
        explorate_rate: 5
        sample_num: 5

inferSent:
    type: InferSent
    model_path: ./other_models/InferSent/infersent1.pkl
    w2v_path: ./other_models/InferSent/inferSent_w2v.pickle
    args:
        bsize: 64
        word_emb_dim: 300
        enc_lstm_dim: 2048
        pool_type: max
        dpout_model: 0.0
        version: 1


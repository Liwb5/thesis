# multi processing with calculating rewards
# using rouge_f to calculate reward
# using tanh and explorate_rate to contrain the result of attention alpha, so that sample can explorate
# loss function can avoid nan value 
# using multi sample instead of exponential moving avg reward
# 0226: add inferSent to calculate reward instead of rouge metric 
# using new embed file: embed_glove.84B.300d.153826w.npz
# 0307: fix the bug when calculate loss function  
# 0307: a new method to calculate loss function which is discovered in banditsum
# 0326: to test explorate_rate
# 0330: to test sample_num 
# 0401: add CNN and daily mail dataset 
# 0402: max_selected number
# 0403: using recall score instead of f-score in ROUGE-L when calculating reward
# 0404: preprocess data in order to remove sentence which length is shorter than 2
task_name: train.recall_maxSelect3_cnndm_sample_num5_rougeReward_embed_300d_explorate_rate3_lr0.0000001.v2_5.big.0404
device: 0    # if you do not want to use GPU, set null
log_level: info 
use_summaryWriter: True
seed: 1667
loss: nll_loss
metrics: rouge_metric

optimizer: 
    type: Adam
    args:
        lr: 0.0000001
        weight_decay: 0.000001
    
trainer:
    type: Trainer
    reward_type: rouge #inferSent   #rouge    # inferSent, rouge 
    args:
        epochs: 30
        save_period: 1
        print_loss_every: 20                   # print train loss every [num] batches, related to the batch size
        print_token_every: 1000000000             # print training tokens to see the result. print every [num] batches 
        print_val_token_every: 500000000         # print val tokens to see the result. print every [num] batches 
        do_validation: True 
        max_norm: 1.0
        epsilon: 0.9    # the initial probability of exploration
        save_dir: checkpoints/
        log_dir: logs/
        teacher_forcing_ratio: 0.9
        output_dir: outputs/

data_loader:
    # train_data: ./data/dm_data_from_summaRuNNer/origin_data/train.json
    # val_data: ./data/dm_data_from_summaRuNNer/origin_data/val.json
    # test_data: ./data/dm_data_from_summaRuNNer/origin_data/test.json
    train_data: ./data/cnn-dailymail/finished_files3/train.json
    val_data: ./data/cnn-dailymail/finished_files3/val.json
    test_data: ./data/cnn-dailymail/finished_files3/test.json
    batch_size: 20
    shuffle: True
    val_data_quota: -1
    data_quota: -1


vocabulary: 
    vocab_file: ./data/dm_data_from_summaRuNNer/origin_data/w2i.json
    embed_file: ./data/dm_data_from_summaRuNNer/origin_data/embed_glove.840B.300d.153826w.npz
    sent_trunc: 120
    doc_trunc: 50


lr_scheduler: 
    type: StepLR
    args: 
        step_size: 50
        gamma: 0.1
    
model: 
    type: RL_AE
    args: 
        embed_dim: 300
        vocab_size: 153826
        hidden_size: 200
        max_len: 120
        sos_id: 2
        eos_id: 3
        max_selected: 3
        rnn_cell: gru
        input_dropout_p: 0.0 
        dropout_p: 0.0 
        bidirectional: True
        use_attention: False
        select_mode: distribute 
        explorate_rate: 3
        sample_num: 5

inferSent:
    type: InferSent
    model_path: ./other_models/InferSent/infersent1.pkl
    w2v_path: ./other_models/InferSent/inferSent_w2v.pickle
    args:
        bsize: 64
        word_emb_dim: 300
        enc_lstm_dim: 2048
        pool_type: max
        dpout_model: 0.0
        version: 1

